# 확률적 경사 하강법 (SGD)

확률적 경사 하강법 (SGD) 은 선형 모델을 학습시키는 간단하지만 효율적인 방법입니다. 특히 샘플과 특징의 수가 매우 많을 때 유용합니다. SGD 는 각 반복에서 학습 데이터의 작은 부분집합을 사용하여 모델 매개변수를 업데이트합니다. 이는 온라인 학습 및 외부 메모리 학습에 적합합니다.

SGD 를 사용하여 로지스틱 회귀 모델을 적합해 보겠습니다.

```python
clf = linear_model.SGDClassifier(loss="log_loss", max_iter=1000)
clf.fit(X, y)

print(clf.coef_)
```

- 로지스틱 회귀를 수행하기 위해 `loss` 매개변수를 "log_loss"로 설정한 `SGDClassifier`의 인스턴스를 생성합니다.
- `fit` 메서드를 사용하여 모델을 학습 데이터에 적합시킵니다.
- SGD 를 사용하여 얻은 로지스틱 회귀 모델의 계수를 출력합니다.
