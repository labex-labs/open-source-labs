# 確率的勾配降下法 (SGD)

確率的勾配降下法 (SGD) は、線形モデルを訓練するためのシンプルで効率的なアプローチです。サンプル数と特徴数が非常に多い場合に特に役立ちます。SGD は、各反復で訓練データの小さなサブセットを使用してモデルパラメータを更新するため、オンライン学習やアウトオブコア学習に適しています。

SGD を使ってロジスティック回帰モデルをフィッティングしてみましょう。

```python
clf = linear_model.SGDClassifier(loss="log_loss", max_iter=1000)
clf.fit(X, y)

print(clf.coef_)
```

- ロジスティック回帰を行うために、`loss` パラメータを "log_loss" に設定して `SGDClassifier` のインスタンスを作成します。
- `fit` メソッドを使ってモデルを訓練データにフィッティングします。
- SGD を使用して得られたロジスティック回帰モデルの係数を表示します。
