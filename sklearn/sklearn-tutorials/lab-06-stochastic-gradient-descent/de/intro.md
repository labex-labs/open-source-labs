# Einführung

Der stochastische Gradientenabstieg (SGD) ist ein populärer Optimierungsalgorithmus, der in der Maschinellen Lernung verwendet wird. Es ist eine Variante des Gradientenabstieg-Algorithmus, der bei jeder Iteration eine zufällig ausgewählte Teilmenge der Trainingsdaten verwendet. Dies macht ihn rechentechnisch effizient und geeignet für die Verarbeitung großer Datensätze. In diesem Lab werden wir die Schritte zum Implementieren von SGD in Python mit scikit-learn durchgehen.

## VM-Tipps

Nachdem der VM-Start abgeschlossen ist, klicken Sie in der oberen linken Ecke, um zur Registerkarte **Notebook** zu wechseln und Jupyter Notebook für die Übung zu nutzen.

Manchmal müssen Sie einige Sekunden warten, bis Jupyter Notebook vollständig geladen ist. Die Validierung von Vorgängen kann aufgrund der Einschränkungen in Jupyter Notebook nicht automatisiert werden.

Wenn Sie bei der Lernphase Probleme haben, können Sie Labby gerne fragen. Geben Sie nach der Sitzung Feedback, und wir werden das Problem für Sie prompt beheben.