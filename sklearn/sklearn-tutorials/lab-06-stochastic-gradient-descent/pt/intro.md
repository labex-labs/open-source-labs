# Introdução

O Gradiente Descendente Estocástico (SGD) é um algoritmo de otimização popular usado no aprendizado de máquina. É uma variação do algoritmo de gradiente descendente que utiliza um subconjunto aleatoriamente selecionado dos dados de treinamento em cada iteração. Isso o torna computacionalmente eficiente e adequado para lidar com grandes conjuntos de dados. Neste laboratório, iremos percorrer os passos da implementação do SGD em Python usando o scikit-learn.

## Dicas da Máquina Virtual

Após o término da inicialização da máquina virtual, clique no canto superior esquerdo para mudar para a aba **Notebook** para acessar o [Jupyter Notebook](https://support.labex.io/en/labex-vm/jupyter) para praticar.

Às vezes, pode ser necessário aguardar alguns segundos para que o Jupyter Notebook termine de carregar. A validação das operações não pode ser automatizada devido a limitações no Jupyter Notebook.

Se você enfrentar problemas durante o aprendizado, sinta-se à vontade para perguntar ao Labby. Forneça feedback após a sessão e resolveremos o problema rapidamente para você.
