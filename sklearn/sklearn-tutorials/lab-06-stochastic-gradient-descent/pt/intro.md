# Introdução

O Gradiente Descendente Estocástico (SGD) é um algoritmo de otimização popular usado no aprendizado de máquina. É uma variação do algoritmo de gradiente descendente que utiliza um subconjunto aleatoriamente selecionado dos dados de treinamento em cada iteração. Isso o torna computacionalmente eficiente e adequado para lidar com grandes conjuntos de dados. Neste laboratório, iremos percorrer os passos da implementação do SGD em Python usando o scikit-learn.

## Dicas da Máquina Virtual

Após o término da inicialização da máquina virtual, clique no canto superior esquerdo para mudar para a aba **Notebook** para acessar o [Jupyter Notebook](https://support.labex.io/en/labex-vm/jupyter) para praticar.

Às vezes, pode ser necessário aguardar alguns segundos para que o Jupyter Notebook termine de carregar. A validação das operações não pode ser automatizada devido a limitações no Jupyter Notebook.

Se você enfrentar problemas durante o aprendizado, sinta-se à vontade para perguntar ao Labby. Forneça feedback após a sessão e resolveremos o problema rapidamente para você.

<div class="text-xs text-gray-500 dark:text-gray-400 mt-4 border-t border-l-2 border-gray-300 dark:border-gray-600 pt-2 pl-4">
Este é um Lab Guiado, que fornece instruções passo a passo para ajudá-lo a aprender e praticar. Siga as instruções cuidadosamente para completar cada etapa e ganhar experiência prática. Dados históricos mostram que este é um laboratório de nível <span class="text-green-600 dark:text-green-400">iniciante</span> com uma taxa de conclusão de <span class="text-green-600 dark:text-green-400">82%</span>. Recebeu uma taxa de avaliações positivas de <span class="text-primary-600 dark:text-primary-400">100%</span> dos estudantes.
</div>
