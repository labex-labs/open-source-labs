# Introduction

La descente de gradient stochastique (SGD) est un algorithme d'optimisation populaire utilisé en apprentissage automatique. C'est une variante de l'algorithme de descente de gradient qui utilise un sous-ensemble aléatoire des données d'entraînement à chaque itération. Cela en fait un algorithme efficace en termes de calcul et adapté à la manipulation de grands ensembles de données. Dans ce laboratoire, nous allons parcourir les étapes de la mise en œuvre de la SGD en Python à l'aide de scikit-learn.

## Conseils sur la VM

Une fois le démarrage de la VM terminé, cliquez dans le coin supérieur gauche pour basculer vers l'onglet **Notebook** pour accéder à Jupyter Notebook pour la pratique.

Parfois, vous devrez peut-être attendre quelques secondes pour que Jupyter Notebook ait fini de charger. La validation des opérations ne peut pas être automatisée en raison des limitations de Jupyter Notebook.

Si vous rencontrez des problèmes pendant l'apprentissage, n'hésitez pas à demander à Labby. Donnez des commentaires après la session, et nous réglerons rapidement le problème pour vous.
