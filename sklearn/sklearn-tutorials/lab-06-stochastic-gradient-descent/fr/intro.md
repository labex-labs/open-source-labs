# Introduction

La descente de gradient stochastique (SGD) est un algorithme d'optimisation populaire utilisé en apprentissage automatique. C'est une variante de l'algorithme de descente de gradient qui utilise un sous-ensemble aléatoire des données d'entraînement à chaque itération. Cela en fait un algorithme efficace en termes de calcul et adapté à la manipulation de grands ensembles de données. Dans ce laboratoire, nous allons parcourir les étapes de la mise en œuvre de la SGD en Python à l'aide de scikit-learn.

## Conseils sur la VM

Une fois le démarrage de la VM terminé, cliquez dans le coin supérieur gauche pour basculer vers l'onglet **Notebook** pour accéder à Jupyter Notebook pour la pratique.

Parfois, vous devrez peut-être attendre quelques secondes pour que Jupyter Notebook ait fini de charger. La validation des opérations ne peut pas être automatisée en raison des limitations de Jupyter Notebook.

Si vous rencontrez des problèmes pendant l'apprentissage, n'hésitez pas à demander à Labby. Donnez des commentaires après la session, et nous réglerons rapidement le problème pour vous.

<div class="text-xs text-gray-500 dark:text-gray-400 mt-4 border-t border-l-2 border-gray-300 dark:border-gray-600 pt-2 pl-4">
Ceci est un Guided Lab, qui fournit des instructions étape par étape pour vous aider à apprendre et à pratiquer. Suivez attentivement les instructions pour compléter chaque étape et acquérir une expérience pratique. Les données historiques montrent que c'est un laboratoire de niveau <span class="text-green-600 dark:text-green-400">débutant</span> avec un taux de réussite de <span class="text-green-600 dark:text-green-400">82%</span>. Il a reçu un taux d'avis positifs de <span class="text-primary-600 dark:text-primary-400">100%</span> de la part des apprenants.
</div>
