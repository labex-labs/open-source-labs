# Введение

Стохастический градиентный спуск (Stochastic Gradient Descent, SGD) — популярный алгоритм оптимизации, используемый в машинном обучении. Это вариант алгоритма градиентного спуска, который на каждой итерации использует случайно выбранное подмножество обучающих данных. Это делает его вычислительно эффективным и подходящим для обработки больших наборов данных. В этом практическом занятии (лабораторной работе) мы рассмотрим шаги по реализации SGD на Python с использованием библиотеки scikit-learn.

## Советы по виртуальной машине (VM)

После запуска виртуальной машины нажмите в левом верхнем углу, чтобы переключиться на вкладку **Notebook** и получить доступ к Jupyter Notebook для практики.

Иногда вам может потребоваться подождать несколько секунд, пока Jupyter Notebook загрузится. Валидация операций не может быть автоматизирована из-за ограничений Jupyter Notebook.

Если у вас возникнут проблемы во время обучения, не стесняйтесь обращаться к Labby. Оставьте отзыв после занятия, и мы оперативно решим проблему для вас.

<div class="text-xs text-gray-500 dark:text-gray-400 mt-4 border-t border-l-2 border-gray-300 dark:border-gray-600 pt-2 pl-4">
Это Guided Lab, который предоставляет пошаговые инструкции, чтобы помочь вам учиться и практиковаться. Внимательно следуйте инструкциям, чтобы выполнить каждый шаг и получить практический опыт. Исторические данные показывают, что это лабораторная работа уровня <span class="text-green-600 dark:text-green-400">начальный</span> с процентом завершения <span class="text-green-600 dark:text-green-400">82%</span>. Он получил <span class="text-primary-600 dark:text-primary-400">100%</span> положительных отзывов от учащихся.
</div>
