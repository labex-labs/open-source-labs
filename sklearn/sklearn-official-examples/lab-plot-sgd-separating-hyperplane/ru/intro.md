# Введение

В этом лабораторном занятии мы научимся использовать метод опорных векторов (Support Vector Machines, SVM) с стохастическим градиентным спуском (Stochastic Gradient Descent, SGD) для классификации данных. SVM - это мощный алгоритм классификации, который широко используется в машинном обучении для классификации и анализа регрессии. Основная идея метода опорных векторов заключается в нахождении наилучшего разделяющего гиперплоскости, которая разделяет данные на классы с максимально возможным отступом. Отступ - это расстояние между гиперплоскостью и ближайшими к ней точками данных из каждого класса. Стохастический градиентный спуск (SGD) - это алгоритм оптимизации, который используется для нахождения наилучших параметров для алгоритма SVM.

## Советы по работе с ВМ

После запуска виртуальной машины (VM) кликните в левом верхнем углу, чтобы переключиться на вкладку **Notebook** и приступить к практике с использованием Jupyter Notebook.

Иногда может потребоваться подождать несколько секунд, пока Jupyter Notebook полностью загрузится. Проверка операций не может быть автоматизирована из-за ограничений Jupyter Notebook.

Если вы сталкиваетесь с проблемами во время обучения, не стесняйтесь обращаться к Labby. Оставьте отзыв после занятия, и мы оперативно решим проблему для вас.
