# 소개

확률적 경사 하강법 (Stochastic Gradient Descent) 은 손실 함수를 최소화하는 데 사용되는 인기 있는 최적화 기법입니다. 이 기법은 각 반복에서 샘플을 무작위로 선택하여 (즉, 확률적으로) 단계적으로 경사 하강법을 수행합니다. 이 방법은 특히 선형 모델을 맞추는 데 효율적입니다. 그러나 각 반복에서 수렴이 보장되지 않으며 손실 함수가 반드시 각 반복에서 감소하지 않을 수 있습니다. 이 경우 손실 함수의 수렴을 모니터링하는 것이 어려울 수 있습니다. 이 실습에서는 검증 점수에 대한 수렴을 모니터링하는 방법인 조기 종료 전략을 탐색할 것입니다. `SGDClassifier` 모델과 MNIST 데이터 세트를 사용하여 조기 종료가 조기 종료를 사용하지 않고 구축된 모델과 거의 동일한 정확도를 달성하고 훈련 시간을 상당히 줄일 수 있는 방법을 보여줄 것입니다.

## VM 팁

VM 시작이 완료되면 왼쪽 상단 모서리를 클릭하여 **Notebook** 탭으로 전환하여 연습을 위한 [Jupyter Notebook](https://support.labex.io/en/labex-vm/jupyter)에 접근할 수 있습니다.

때때로 Jupyter Notebook 이 완전히 로드되기까지 몇 초 정도 기다려야 할 수 있습니다. Jupyter Notebook 의 제한으로 인해 작업의 유효성 검사를 자동화할 수 없습니다.

학습 중 문제가 발생하면 Labby 에 문의하십시오. 세션 후 피드백을 제공하면 문제를 신속하게 해결해 드리겠습니다.
