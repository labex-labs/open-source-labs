# Введение

Стохастический градиентный спуск - это популярный метод оптимизации, используемый для минимизации функции потерь. Метод выполняет градиентный спуск шаг за шагом по-стахстически, то есть при каждой итерации случайным образом выбирает выборки. Метод эффективен, особенно при подгонке линейных моделей. Однако гарантии сходимости на каждой итерации нет, и функция потерь не обязательно уменьшается на каждой итерации. В этом случае отслеживание сходимости по функции потерь может быть сложным. В этом лабе мы исследуем стратегию ранней остановки, которая представляет собой подход для отслеживания сходимости по валидационному показателю. Мы будем использовать модель `SGDClassifier` из библиотеки scikit-learn и датасет MNIST, чтобы показать, как ранняя остановка может быть использована для достижения практически той же точности, что и у модели, построенной без ранней остановки, и значительного сокращения времени обучения.

## Советы по работе с ВМ

После запуска ВМ кликните в верхнем левом углу, чтобы переключиться на вкладку **Notebook** и получить доступ к Jupyter Notebook для практики.

Иногда вам может потребоваться подождать несколько секунд, пока Jupyter Notebook загрузится. Валидация операций не может быть автоматизирована из-за ограничений Jupyter Notebook.

Если вы сталкиваетесь с проблемами во время обучения, не стесняйтесь обращаться к Labby. Оставьте отзыв после занятия, и мы оперативно решим проблему для вас.
