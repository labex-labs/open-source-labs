# Resumo

Neste laboratório, exploramos a estratégia de parada antecipada para monitorar a convergência em uma pontuação de validação ao usar o Gradiente Descendente Estocástico para minimizar uma função de perda. Usamos o modelo `SGDClassifier` do scikit-learn e o conjunto de dados MNIST para ilustrar como a parada antecipada pode ser usada para atingir quase a mesma precisão em comparação com um modelo construído sem parada antecipada, e reduzir significativamente o tempo de treinamento. Definimos três critérios de parada diferentes: nenhum critério de parada, perda de treinamento e pontuação de validação, e usamos um loop para treinar e avaliar o estimador usando cada critério de parada. Em seguida, plotamos os resultados usando estilos de linha diferentes para cada estimador e critério de parada.
