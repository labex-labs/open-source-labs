# はじめに

確率的勾配降下法（Stochastic Gradient Descent）は、損失関数を最小化するために使用される一般的な最適化手法です。この手法は、確率的に各反復でサンプルを選択することで、勾配降下法を段階的に実行します。この方法は、特に線形モデルのフィッティングに効率的です。ただし、各反復で収束が保証されるわけではなく、各反復で損失関数が必ずしも減少するとは限りません。この場合、損失関数の収束を監視するのは難しい場合があります。この実験では、検証スコアの収束を監視するアプローチである早期終了戦略を探ります。scikit-learnライブラリの`SGDClassifier`モデルとMNISTデータセットを使用して、早期終了を使用した場合と早期終了を使用せずに構築したモデルと比較して、ほぼ同じ精度を達成し、学習時間を大幅に短縮できる方法を示します。

## VMのヒント

VMの起動が完了したら、左上隅をクリックして**ノートブック**タブに切り替え、Jupyter Notebookを使って練習しましょう。

時々、Jupyter Notebookが読み込み終了するまで数秒待つ必要がある場合があります。Jupyter Notebookの制限により、操作の検証を自動化することはできません。

学習中に問題に遭遇した場合は、Labbyにお問い合わせください。セッション後にフィードバックを提供してください。すぐに問題を解決いたします。
