# Resumen

En este laboratorio, exploramos la estrategia de parada temprana para monitorear la convergencia en una puntuación de validación al utilizar el Descenso de Gradiente Estocástico para minimizar una función de pérdida. Utilizamos el modelo `SGDClassifier` de scikit-learn y el conjunto de datos MNIST para ilustrar cómo la parada temprana se puede utilizar para alcanzar una precisión casi igual a la de un modelo construido sin parada temprana, y reducir significativamente el tiempo de entrenamiento. Definimos tres criterios de parada diferentes: sin criterio de parada, pérdida de entrenamiento y puntuación de validación, y utilizamos un bucle para entrenar y evaluar el estimador utilizando cada criterio de parada. Luego representamos los resultados utilizando diferentes estilos de línea para cada estimador y criterio de parada.
