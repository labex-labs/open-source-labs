# Introdução

Em aprendizagem de máquina, hiperparâmetros são parâmetros que não são aprendidos a partir dos dados, mas sim definidos antes do treino. A seleção de hiperparâmetros apropriados é crucial para alcançar alta precisão em modelos de aprendizagem de máquina. Dois métodos comuns para otimização de hiperparâmetros são a busca aleatória e a busca em grade. Neste laboratório, compararemos esses dois métodos para otimizar os hiperparâmetros de uma Máquina de Vetores de Suporte (SVM) linear com treino por Gradiente Descendente Estocástico (SGD).

## Dicas da Máquina Virtual

Após o arranque da máquina virtual, clique no canto superior esquerdo para mudar para a aba **Notebook** para aceder ao [Jupyter Notebook](https://support.labex.io/en/labex-vm/jupyter) para a prática.

Por vezes, pode ser necessário esperar alguns segundos para o Jupyter Notebook terminar de carregar. A validação das operações não pode ser automatizada devido a limitações no Jupyter Notebook.

Se tiver problemas durante o aprendizado, não hesite em contactar o Labby. Forneça feedback após a sessão e resolveremos prontamente o problema para si.
