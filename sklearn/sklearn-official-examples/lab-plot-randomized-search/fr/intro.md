# Introduction

En apprentissage automatique, les hyperparamètres sont des paramètres qui ne sont pas appris à partir des données, mais plutôt définis avant l'entraînement. La sélection d'hyperparamètres appropriés est cruciale pour atteindre une haute précision dans les modèles d'apprentissage automatique. Deux méthodes courantes d'optimisation d'hyperparamètres sont la recherche aléatoire et la recherche en grille. Dans ce laboratoire, nous comparerons ces deux méthodes pour optimiser les hyperparamètres d'un Support Vector Machine (SVM) linéaire avec entraînement par Descente de Gradient Stochastique (SGD).

## Conseils sur la machine virtuelle

Une fois le démarrage de la machine virtuelle terminé, cliquez dans le coin supérieur gauche pour basculer vers l'onglet **Notebook** pour accéder à Jupyter Notebook pour la pratique.

Parfois, vous devrez peut-être attendre quelques secondes pour que Jupyter Notebook ait fini de charger. La validation des opérations ne peut pas être automatisée en raison des limites de Jupyter Notebook.

Si vous rencontrez des problèmes pendant l'apprentissage, n'hésitez pas à demander à Labby. Donnez votre feedback après la session, et nous résoudrons rapidement le problème pour vous.
