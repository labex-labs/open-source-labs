# Sommaire

Dans ce laboratoire, nous avons exploré les limites théoriques du lemme de Johnson-Lindenstrauss pour l'imbrication avec des projections aléatoires et le validé empiriquement à l'aide de scikit-learn en Python. Nous avons tracé le nombre minimum de dimensions nécessaire pour garantir un plongement `eps` pour un nombre croissant d'échantillons `n_samples`. Nous avons également validé empiriquement les bornes de Johnson-Lindenstrauss sur l'ensemble de données de documents texte 20 newsgroups ou sur l'ensemble de données des chiffres. Nous avons projeté 300 documents avec 100 000 caractéristiques au total en utilisant une matrice aléatoire sparse vers des espaces euclidiens plus petits avec diverses valeurs pour le nombre cible de dimensions `n_components`. Nous pouvons constater que pour de faibles valeurs de `n_components`, la distribution est large avec de nombreux paires distordues et une distribution asymétrique, tandis que pour de plus grandes valeurs de `n_components`, la distorsion est contrôlée et les distances sont bien conservées par la projection aléatoire.
