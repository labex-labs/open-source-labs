# Zusammenfassung

In diesem Lab haben wir die theoretischen Grenzen des Johnson-Lindenstrauss-Lemmas für die Einbettung mit zufälligen Projekten untersucht und es empirisch mit Python scikit-learn validiert. Wir haben die minimale Anzahl an Dimensionen geplottet, die erforderlich sind, um eine `eps`-Einbettung für eine zunehmende Anzahl von Proben `n_samples` zu gewährleisten. Wir haben auch die Johnson-Lindenstrauss-Grenzen empirisch auf dem 20 Newsgroups-Text-Dokumentendataset oder auf dem Digits-Dataset validiert. Wir haben 300 Dokumente mit insgesamt 100.000 Merkmalen mithilfe einer dünnen Zufallsmatrix in kleinere euklidische Räume mit verschiedenen Werten für die Zielanzahl der Dimensionen `n_components` projiziert. Wir können sehen, dass für niedrige Werte von `n_components` die Verteilung weit ist mit vielen verzerrten Paaren und einer schiefen Verteilung, während für größere Werte von `n_components` die Verzerrung kontrolliert ist und die Distanzen durch die zufällige Projekion gut erhalten bleiben.
