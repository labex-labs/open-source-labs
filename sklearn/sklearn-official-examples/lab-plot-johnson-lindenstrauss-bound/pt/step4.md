# Resumo

Neste laboratório, exploramos os limites teóricos do lema de Johnson-Lindenstrauss para incorporação com projeções aleatórias e o validamos empiricamente usando Python scikit-learn. Plotamos o número mínimo de dimensões necessárias para garantir uma incorporação `eps` para um número crescente de amostras `n_samples`. Também validamos os limites de Johnson-Lindenstrauss empiricamente no conjunto de dados de documentos de texto 20 newsgroups ou no conjunto de dados de dígitos. Projetamos 300 documentos com 100k recursos no total usando uma matriz aleatória esparsa para espaços euclidianos menores com vários valores para o número alvo de dimensões `n_components`. Podemos observar que para valores baixos de `n_components`, a distribuição é ampla, com muitos pares distorcidos e uma distribuição assimétrica, enquanto para valores maiores de `n_components`, a distorção é controlada e as distâncias são bem preservadas pela projeção aleatória.
