# 소개

이 실습에서는 scikit-learn 을 사용하여 Gradient Boosting 에 대한 다양한 정규화 전략을 구현하는 방법을 배웁니다. 정규화는 머신러닝 모델에서 흔히 발생하는 과적합을 방지하는 데 도움이 되는 기술입니다. 이 실습에서는 이항 편차 손실 함수 (binomial deviance loss function) 와 make_hastie_10_2 데이터셋을 사용합니다.

## VM 팁

VM 시작이 완료되면 왼쪽 상단 모서리를 클릭하여 **Notebook** 탭으로 전환하여 연습을 위한 [Jupyter Notebook](https://support.labex.io/en/labex-vm/jupyter)에 접근합니다.

때때로 Jupyter Notebook 이 완전히 로드되기까지 몇 초 정도 기다려야 할 수 있습니다. Jupyter Notebook 의 제한으로 인해 작업 검증을 자동화할 수 없습니다.

학습 중 문제가 발생하면 Labby 에게 문의하십시오. 세션 후 피드백을 제공하면 문제를 신속하게 해결해 드리겠습니다.
