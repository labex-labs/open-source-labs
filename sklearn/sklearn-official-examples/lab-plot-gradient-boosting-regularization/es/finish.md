# Resumen

En este laboratorio, aprendimos cómo implementar diferentes estrategias de regularización para Gradient Boosting utilizando scikit-learn. Utilizamos la función de pérdida de desviación binomial y el conjunto de datos make_hastie_10_2. Implementamos diferentes estrategias de regularización como no contracción, tasa de aprendizaje = 0.2, submuestreo = 0.5 y máximo de características = 2. Finalmente, graficamos la desviación del conjunto de prueba para cada estrategia de regularización para comparar su rendimiento.
