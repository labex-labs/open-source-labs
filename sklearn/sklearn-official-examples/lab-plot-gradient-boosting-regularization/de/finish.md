# Zusammenfassung

In diesem Lab haben wir gelernt, wie man verschiedene Regularisierungstrategien für Gradient Boosting mit scikit-learn implementiert. Wir haben die binomiale Abweichungsverlustfunktion und den make_hastie_10_2-Datensatz verwendet. Wir haben verschiedene Regularisierungstrategien implementiert, wie z. B. keine Schrumpfung, Lernrate = 0,2, Subsample = 0,5 und maximale Anzahl von Merkmalen = 2. Schließlich haben wir die Testset-Abweichung für jede Regularisierungstrategie geplottet, um deren Leistung zu vergleichen.
