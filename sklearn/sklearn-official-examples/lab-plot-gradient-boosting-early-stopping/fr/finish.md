# Sommaire

Dans ce laboratoire, nous avons appris à propos de l'arrêt précoce dans le gradient boosting, qui nous permet de trouver le nombre minimal d'itérations suffisant pour construire un modèle qui généralise bien aux données non vues. Nous avons comparé les performances d'un modèle de gradient boosting avec et sans arrêt précoce, et avons observé que l'arrêt précoce peut réduire considérablement le temps d'entraînement, la consommation de mémoire et la latence de prédiction.
