# Introducción

El gradient boosting es una técnica de ensamble en la que varios weak learners (árboles de regresión) se combinan para producir un solo modelo potente, de manera iterativa. La compatibilidad con early stopping en Gradient Boosting nos permite encontrar el menor número de iteraciones que es suficiente para construir un modelo que se generalice bien a datos no vistos.

## Consejos sobre la VM

Una vez finalizada la inicialización de la VM, haga clic en la esquina superior izquierda para cambiar a la pestaña **Notebook** y acceder a Jupyter Notebook para practicar.

A veces, es posible que tenga que esperar unos segundos a que Jupyter Notebook termine de cargarse. La validación de operaciones no se puede automatizar debido a las limitaciones de Jupyter Notebook.

Si tiene problemas durante el aprendizaje, no dude en preguntar a Labby. Deje su retroalimentación después de la sesión y le resolveremos el problema inmediatamente.
