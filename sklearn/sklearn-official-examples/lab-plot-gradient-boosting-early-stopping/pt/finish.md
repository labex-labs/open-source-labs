# Resumo

Neste laboratório, aprendemos sobre a parada antecipada em boosting de gradiente, que nos permite encontrar o menor número de iterações suficientes para construir um modelo que generalize bem para dados não vistos. Comparamos o desempenho de um modelo de boosting de gradiente com e sem parada antecipada e observamos que a parada antecipada pode reduzir significativamente o tempo de treinamento, o uso de memória e a latência de previsão.
