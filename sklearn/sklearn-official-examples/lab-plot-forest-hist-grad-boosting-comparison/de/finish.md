# Zusammenfassung

In diesem Lab haben wir die Leistung zweier populärer Ensemble-Modelle, Random Forest und Histogram Gradient Boosting, für einen Regressionsdatensatz in Bezug auf Score und Rechenzeit verglichen. Wir haben die Parameter variiert, die die Anzahl der Bäume je nach Schätzer steuern, und die Ergebnisse geplottet, um das Kompromissverhältnis zwischen der vergangenen Rechenzeit und dem mittleren Testscore zu visualisieren. Wir haben beobachtet, dass HGBT-Modelle die RF-Modelle einheitlich in der "Testscore-Gegenüber-Trainingsgeschwindigkeit-Kompromissanalyse" dominieren und der "Testscore-Gegenüber-Vorhersagegeschwindigkeit-Kompromiss" auch günstiger für HGBT sein kann. HGBT bietet fast immer einen günstigeren Geschwindigkeit-Genauigkeit-Kompromiss als RF, entweder mit den standardmäßigen Hyperparametern oder unter Berücksichtigung der Kosten der Hyperparameteroptimierung.
