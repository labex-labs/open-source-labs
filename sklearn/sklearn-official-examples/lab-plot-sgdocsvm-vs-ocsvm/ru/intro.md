# Введение

В этом практическом занятии мы покажем, как использовать стохастический градиентный спуск (Stochastic Gradient Descent, SGD) для приближения решения задачи One-Class SVM в случае использования RBF-ядра.

Мы сравним результаты этого приближения с результатами, полученными при использовании One-Class SVM с ядровым подходом. Цель этого практического занятия не показать преимущества приближения по времени вычислений, а показать, что можно получить похожие результаты с использованием SGD на небольшом наборе данных.

## Советы по использованию ВМ

После запуска ВМ нажмите в левом верхнем углу, чтобы переключиться на вкладку **Notebook** и получить доступ к Jupyter Notebook для практики.

Иногда может потребоваться подождать несколько секунд, пока Jupyter Notebook полностью загрузится. Валидация операций не может быть автоматизирована из-за ограничений Jupyter Notebook.

Если вы сталкиваетесь с проблемами во время обучения, не стесняйтесь задавать вопросы Labby. Оставьте отзыв после занятия, и мы оперативно решим проблему для вас.
