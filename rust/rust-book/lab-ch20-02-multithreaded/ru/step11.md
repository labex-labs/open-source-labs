# Реализация метода execute

Наконец, реализуем метод `execute` для `ThreadPool`. Также изменим `Job` с структуры на псевдоним типа для объекта трейта, который хранит тип замыкания, которое получает `execute`. Как обсуждалось в разделе "Создание псевдонимов типа с использованием псевдонимов типа", псевдонимы типов позволяют нам сократить длинные типы для удобства использования. Посмотрите на Listing 20-19.

Filename: `src/lib.rs`

```rust
--snip--

type Job = Box<dyn FnOnce() + Send + 'static>;

impl ThreadPool {
    --snip--

    pub fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
      1 let job = Box::new(f);

      2 self.sender.send(job).unwrap();
    }
}

--snip--
```

Listing 20-19: Создание псевдонима типа `Job` для `Box`, которая хранит каждое замыкание, а затем отправляет задачу по каналу

После создания нового экземпляра `Job` с использованием замыкания, которое мы получаем в `execute` \[1\], мы отправляем эту задачу по отправной стороне канала \[2\]. Мы вызываем `unwrap` для `send` в случае, если отправка не удалась. Это может произойти, если, например, мы остановим все наши потоки от выполнения, что означает, что приемная сторона перестала получать новые сообщения. В данный момент мы не можем остановить наши потоки от выполнения: наши потоки продолжают выполняться, пока пул существует. Причина, по которой мы используем `unwrap`, заключается в том, что мы знаем, что случай неудачи не произойдет, но компилятор это не знает.

Но мы еще не закончили! В `Worker` наше замыкание, передаваемое в `thread::spawn`, по-прежнему только _ссылается_ на приемную сторону канала. Вместо этого нам нужно, чтобы замыкание циклически запрашивало задачу у приемной стороны канала и выполняло задачу, когда ее получает. Давайте внесем изменения, показанные в Listing 20-20, в `Worker::new`.

Filename: `src/lib.rs`

```rust
--snip--

impl Worker {
    fn new(
        id: usize,
        receiver: Arc<Mutex<mpsc::Receiver<Job>>>,
    ) -> Worker {
        let thread = thread::spawn(move || loop {
            let job = receiver
              1.lock()
              2.unwrap()
              3.recv()
              4.unwrap();

            println!("Worker {id} got a job; executing.");

            job();
        });

        Worker { id, thread }
    }
}
```

Listing 20-20: Прием и выполнение задач в потоке экземпляра `Worker`

Здесь мы сначала вызываем `lock` для `receiver`, чтобы получить мьютекс \[1\], а затем вызываем `unwrap`, чтобы завершить работу с ошибкой при любых ошибках \[2\]. Захват блокировки может завершиться неудачей, если мьютекс находится в _зараженном_ состоянии, что может произойти, если какой-то другой поток завершился с ошибкой, не освободив блокировку. В этом случае вызов `unwrap`, чтобы заставить этот поток завершиться с ошибкой, является правильным действием. Не стесняйтесь заменить это `unwrap` на `expect` с сообщением об ошибке, которое имеет смысл для вас.

Если мы получаем блокировку мьютекса, мы вызываем `recv`, чтобы получить `Job` из канала \[3\]. Последнее `unwrap` также пропускает любые ошибки здесь \[4\], которые могут произойти, если поток, владеющий отправителем, остановился, аналогично тому, как метод `send` возвращает `Err`, если получатель остановился.

Вызов `recv` блокирует выполнение, поэтому если еще нет задачи, текущий поток будет ждать, пока задача не станет доступной. `Mutex<T>` гарантирует, что только один поток `Worker` в данный момент пытается запросить задачу.

Наша пул потоков теперь находится в рабочем состоянии! Запустите `cargo run` и сделайте несколько запросов:

```bash
[object Object]
```

Ура! Теперь у нас есть пул потоков, который асинхронно выполняет соединения. Никогда не создается более четырех потоков, поэтому наша система не будет перегружена, если сервер получает много запросов. Если мы отправим запрос на _/sleep_, сервер сможет обслуживать другие запросы, запустив их в другом потоке.

> Примечание: Если вы одновременно откроете _/sleep_ в нескольких окнах браузера, они могут загружаться по одному с интервалом в пять секунд. Некоторые веб-браузеры последовательно выполняют несколько экземпляров одного и того же запроса по причинам кэширования. Эта ограничение не связано с нашим веб-сервером.

После изучения цикла `while let` в главе 18 вы, возможно, задаетесь вопросом, почему мы не написали код потока `Worker` как показано в Listing 20-21.

Filename: `src/lib.rs`

```rust
--snip--

impl Worker {
    fn new(
        id: usize,
        receiver: Arc<Mutex<mpsc::Receiver<Job>>>,
    ) -> Worker {
        let thread = thread::spawn(move || {
            while let Ok(job) = receiver.lock().unwrap().recv() {
                println!("Worker {id} got a job; executing.");

                job();
            }
        });

        Worker { id, thread }
    }
}
```

Listing 20-21: Альтернативная реализация `Worker::new` с использованием `while let`

Этот код компилируется и работает, но не приводит к желаемому поведению многопоточности: медленный запрос по-прежнему заставит другие запросы ждать обработки. Причина несколько тонкая: структура `Mutex` не имеет публичного метода `unlock`, потому что владение блокировкой основано на времени жизни `MutexGuard<T>` внутри `LockResult<MutexGuard<T>>`, который возвращает метод `lock`. На этапе компиляции проверщик ссылок может затем наложить правило, что ресурс, защищенный мьютексом, нельзя использовать, если мы не держим блокировку. Однако данная реализация также может привести к тому, что блокировка будет удерживаться дольше, чем планируется, если мы не будем обращать внимание на время жизни `MutexGuard<T>`.

Код в Listing 20-20, который использует `let job = receiver.lock().unwrap().recv().unwrap();`, работает потому, что с использованием `let` любые временные значения, используемые в выражении справа от знака равенства, немедленно удаляются, когда заканчивается инструкция `let`. Однако `while let` (и `if let` и `match`) не удаляет временные значения до конца связанного блока. В Listing 20-21 блокировка остается удерживаемой в течение вызова `job()`, что означает, что другие экземпляры `Worker` не могут получать задачи.
